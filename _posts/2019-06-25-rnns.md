---
layout: post
title: "RNNs"
date: 2019-06-25 22:56:57
image: '/assets/img/'
description:
tags:
categories:
twitter_text:
---
# Gated units
As of this writing, the most effective sequence models used in practical applications are called gated RNNs. These include the long short-term memory and
networks based on the gated recurrent unit . Like leaky units, gated RNNs are based on the idea of creating paths through time that have derivatives that neither vanish nor explode. Leaky units did this with connection weights that were either manually chosen constants or were parameters.Leaky units allow the network to accumulate information (such as evidence
for a particular feature or category) over a long duration. However, once that
information has been used, it might be useful for the neural network to forget the old state. For example, if a sequence is made of sub-sequences and we want a leaky unit to accumulate evidence inside each sub-subsequence, we need a mechanism to forget the old state by setting it to zero. Instead of manually deciding when to clear the state, we want the neural network to learn to decide when to do it. This is called gated units
# LSTMs
The clever idea of introducing self-loops to produce paths where the gradient can flow for long durations is a core contribution of the initial long short-term memory (LSTM) model and an important addition is that making the weight of this self looping unit determined by another hidden unit so it's not fixed
![]('/assets/img/lstm.png')
“LSTM cells” that have an internal recurrence (a self-loop),
in addition to the outer recurrence of the RNN. Each cell has the same inputs and outputs as an ordinary recurrent network, but has more parameters and a system of gating units that controls the flow of information The most important component is the state unit s( i ) that has a linear self-loop similar to the leaky units described in the previous section. However, here, the self-loop weight (or the
associated time constant) is controlled by a forget gate unit f ( t )  that sets this weight to a value between 0 and 1 via a sigmoid unit.

## Forget cell equation
![]('/assets/img/forget_eq.png')
This is the unit that controls the weights of the self looping unit we talked about earlier
where x ( t ) is the current input vector and h ( t ) is the current hidden layer vector, containing the outputs of all the LSTM cells, and b f ,U f , W f are respectively biases, input weights and recurrent weights for the forget gates

![]('/assets/img/equ.png')

## Equations governing LSTM.
With all the gates defined, we now develop a LSTM prototype by defining the required behavior. To write a candidate state s(t), we follow a simple rule of thumb.
1. Take the inputs using the write gate
2. Calculate the output using the read gate (output is reading the input information so you can remember that it uses the read gate and not the input)
3. Combine the output with relevant prior information, for keeping relevant information we use the forget gate with prior state.

In theory, this prototype should work but turns out it doesn’t. It happens because, even after well thought initializations and write and forget gates, the coordination between these gates in early stage of training gets tricky and very often it becomes large and chaotic at write step. For more details, refer to “internal state drift” problem, further, an empirical demonstration of this can be found in Greff et al. (2015), which covers 8 variants of LSTMs.
The solution to above problem is bounding the state to prevent it from becoming chaotic or blowing up. There are 3 variants of LSTM which uses this solution 1. Normalized LSTM, GRU and Pseudo LSTM.
